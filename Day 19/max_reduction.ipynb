{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07LqT4qMOqfG",
        "outputId": "6cd407ef-da84-4cdc-dd05-21123df67da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing MaxReduction.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile MaxReduction.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Naive Kernel\n",
        "__global__ void naiveMaxRed(float *input, float *output, int N){\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < N){\n",
        "        float max_val = -INFINITY;\n",
        "        for (int i=idx; i<N; i++){\n",
        "            max_val = max(max_val, input[i]);\n",
        "        }\n",
        "        output[idx] = max_val;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Interleaved Addressing Kernel\n",
        "__global__ void MaxRed1(float *input, float *output, int N){\n",
        "    extern __shared__ float sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // load into shared memory\n",
        "    if (idx < N){\n",
        "        sdata[tid] = input[idx];\n",
        "    } else {\n",
        "        sdata[tid] = -INFINITY;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // do max reduction in shared memory\n",
        "    for (int stride=1; stride < blockDim.x; stride *= 2){\n",
        "        if (tid % (2 * stride) == 0){\n",
        "            sdata[tid] = max(sdata[tid], sdata[tid + stride]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (tid == 0){\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Interleaved Addressing Kernel 2\n",
        "__global__ void MaxRed2(float *input, float *output, int N){\n",
        "    extern __shared__ float sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // load into shared mem\n",
        "    if (idx < N) sdata[tid] = input[idx];\n",
        "    else sdata[tid] = -INFINITY;\n",
        "    __syncthreads();\n",
        "\n",
        "    // max reduction in shared mem\n",
        "    for (int stride=1; stride < blockDim.x ; stride *= 2){\n",
        "        int index = 2 * stride * tid;\n",
        "        if (index < blockDim.x && index + stride < blockDim.x) sdata[index] = max(sdata[index], sdata[index + stride]);\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (tid == 0) output[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "// Sequential Addressing Kernel\n",
        "__global__ void MaxRed3(float *input, float *output, int N){\n",
        "    extern __shared__ float sdata[];\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // load into shared mem\n",
        "    if (idx < N) sdata[tid] = input[idx];\n",
        "    else sdata[tid] = -INFINITY;\n",
        "    __syncthreads();\n",
        "\n",
        "    // max reduction\n",
        "    for (int s = blockDim.x / 2; s > 0; s >>= 1){\n",
        "        if (tid < s){\n",
        "            sdata[tid] = max(sdata[tid], sdata[tid + s]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (tid == 0) output[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "// Thread-Level Addressing\n",
        "__global__ void MaxRed4(float *input, float *output, int N){\n",
        "    extern __shared__ float sdata[];\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    float thread_max = -INFINITY;\n",
        "\n",
        "    // Grid-Stride Loop\n",
        "    for (int i = idx; i < N; i += blockDim.x * gridDim.x){\n",
        "        thread_max = fmaxf(thread_max, input[i]);\n",
        "    }\n",
        "    sdata[tid] = thread_max;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Block-level reduction (Sequential)\n",
        "    for (int s=blockDim.x / 2; s > 0; s >>= 1){\n",
        "        if (tid < s){\n",
        "            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (tid == 0) output[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "// vectorised memory access , warp reduction\n",
        "__inline__ __device__ float warpReduce(float val){\n",
        "    for (int offset=16; offset >0; offset /= 2){\n",
        "        val = fmax(val, __shfl_down_sync(0xffffffff, val, offset));\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "// block reduction\n",
        "__inline__ __device__ float blockReduce(float val, float* shared){\n",
        "    int lane = threadIdx.x % 32;\n",
        "    int wid = threadIdx.x / 32;\n",
        "\n",
        "    // 1. warp-level reduction\n",
        "    val = warpReduce(val);\n",
        "\n",
        "    // 2. write reduced val to shared mem\n",
        "    if (lane == 0) shared[wid] = val;\n",
        "    __syncthreads();\n",
        "\n",
        "    // 3. read back warp results and reduce the last remaining values\n",
        "    if (threadIdx.x < (blockDim.x / 32.0f)) val = shared[lane];\n",
        "    else val = -INFINITY;\n",
        "\n",
        "    if (wid == 0) {\n",
        "        val = warpReduce(val);\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "__global__ void MaxRed5(float *input, float *output, int N){\n",
        "    extern __shared__ float sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float localMax = -INFINITY;\n",
        "\n",
        "    // 1.vectorized grid\n",
        "    int vecN = N / 4;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "    // reinterpret cast\n",
        "    float4* intptr = (float4*)input;\n",
        "    for (int i=idx; i<vecN; i+= stride){\n",
        "        float4 v = intptr[i];\n",
        "        localMax = fmaxf(localMax, v.x);\n",
        "        localMax = fmaxf(localMax, v.y);\n",
        "        localMax = fmaxf(localMax, v.z);\n",
        "        localMax = fmaxf(localMax, v.w);\n",
        "    }\n",
        "    // handle tail elements\n",
        "    for (int i = vecN * 4 + idx; i < N; i += stride){\n",
        "        if (i < N) localMax = fmaxf(localMax, input[i]);\n",
        "    }\n",
        "\n",
        "    // 2. Block-wide reduction using warp shuffle\n",
        "    float blockMax = blockReduce(localMax, sdata);\n",
        "\n",
        "    // 3. write result for this block\n",
        "    if (tid == 0) output[blockIdx.x] = blockMax;\n",
        "}\n",
        "\n",
        "// host code\n",
        "#define CUDA_CHECK(call) \\\n",
        "    do { \\\n",
        "        cudaError_t err = call; \\\n",
        "        if (err != cudaSuccess) { \\\n",
        "            fprintf(stderr, \"CUDA CHECK ERROR %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(EXIT_FAILURE); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "// cpu reference\n",
        "float cpuMax(float *data, int n){\n",
        "    float max_val = -INFINITY;\n",
        "    for (int i=0; i<n; i++){\n",
        "        if (data[i] > max_val) max_val = data[i];\n",
        "    }\n",
        "    return max_val;\n",
        "}\n",
        "\n",
        "// initalize random data\n",
        "void initInput(float *data, int n){\n",
        "    for (int i=0; i<n; i++){\n",
        "        data[i] = (float)(rand() % 100000) / 100.0f;\n",
        "    }\n",
        "    // Manually placing a known maximum to ensure edge cases\n",
        "    data[n/2] = 123456.0f;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int n = 1 << 24;\n",
        "    size_t bytes = n * sizeof(float);\n",
        "\n",
        "    float *h_input = (float*)malloc(bytes);\n",
        "    initInput(h_input, n);\n",
        "\n",
        "    printf(\"Calculating CPU reference for N=%d....\\n\", n);\n",
        "    clock_t start_cpu = clock();\n",
        "    float cpu_result = cpuMax(h_input, n);\n",
        "    clock_t end_cpu = clock();\n",
        "    printf(\"CPU result: %.2f (Time : %.4f of ms)\\n\", cpu_result, (double)(end_cpu - start_cpu)/CLOCKS_PER_SEC * 1000);\n",
        "\n",
        "    // allocate device memory\n",
        "    float *d_input, *d_output;\n",
        "    CUDA_CHECK(cudaMalloc((void**)&d_input, bytes));\n",
        "    CUDA_CHECK(cudaMemcpy(d_input, h_input, bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    int max_grid = (n + 255) / 256;\n",
        "    float *h_output = (float*)malloc(max_grid * 256 * sizeof(float));\n",
        "    CUDA_CHECK(cudaMalloc((void**)&d_output, max_grid * 256 * sizeof(float)));\n",
        "\n",
        "    int blockSize = 256;\n",
        "    int gridSizeStandard = (n + blockSize - 1) / blockSize;\n",
        "    int gridSizePersistent = 2048;\n",
        "\n",
        "    auto runKernel = [&](const char* name, void (*kernel)(float*, float*, int), int grid, int block, int smem, bool isNaive) {\n",
        "\n",
        "        // Clear output\n",
        "        CUDA_CHECK(cudaMemset(d_output, 0, grid * (isNaive ? block : 1) * sizeof(float)));\n",
        "\n",
        "        // Setup Events\n",
        "        cudaEvent_t start, stop;\n",
        "        CUDA_CHECK(cudaEventCreate(&start));\n",
        "        CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "        // Launch\n",
        "        CUDA_CHECK(cudaEventRecord(start));\n",
        "        kernel<<<grid, block, smem>>>(d_input, d_output, n);\n",
        "        CUDA_CHECK(cudaEventRecord(stop));\n",
        "        CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "        // Timing\n",
        "        float milliseconds = 0;\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "\n",
        "        // Copy Result\n",
        "        int output_count = isNaive ? (grid * block) : grid;\n",
        "        CUDA_CHECK(cudaMemcpy(h_output, d_output, output_count * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "        // Final CPU Reduction of partial sums\n",
        "        float gpu_result = -INFINITY;\n",
        "        for(int i=0; i<output_count; i++){\n",
        "            gpu_result = fmaxf(gpu_result, h_output[i]);\n",
        "        }\n",
        "\n",
        "        // Verify\n",
        "        bool match = fabsf(gpu_result - cpu_result) < 1e-4; // float precision tolerance\n",
        "        printf(\"%-25s | Grid: %4d | Time: %6.3f ms | Result: %.2f | %s\\n\",\n",
        "               name, grid, milliseconds, gpu_result, match ? \"PASS\" : \"FAIL\");\n",
        "\n",
        "        CUDA_CHECK(cudaEventDestroy(start));\n",
        "        CUDA_CHECK(cudaEventDestroy(stop));\n",
        "    };\n",
        "    printf(\"\\nKernel Performance Comparison:\\n\");\n",
        "    printf(\"--------------------------------------------------------------------------------\\n\");\n",
        "\n",
        "    // RUN TESTS\n",
        "    // 1. Naive (Grid Stride)\n",
        "    runKernel(\"Naive (Grid Stride)\", naiveMaxRed, gridSizePersistent, blockSize, 0, true);\n",
        "\n",
        "    // 2. Interleaved 1 (Standard Grid)\n",
        "    runKernel(\"Interleaved 1 (Mod)\", MaxRed1, gridSizeStandard, blockSize, blockSize * sizeof(float), false);\n",
        "\n",
        "    // 3. Interleaved 2 (Standard Grid)\n",
        "    runKernel(\"Interleaved 2 (Strided)\", MaxRed2, gridSizeStandard, blockSize, blockSize * sizeof(float), false);\n",
        "\n",
        "    // 4. Sequential (Standard Grid)\n",
        "    runKernel(\"Sequential\", MaxRed3, gridSizeStandard, blockSize, blockSize * sizeof(float), false);\n",
        "\n",
        "    // 5. Thread-Level (Grid Stride + Sequential Red)\n",
        "    runKernel(\"MaxRed4 (Grid Stride)\", MaxRed4, gridSizePersistent, blockSize, blockSize * sizeof(float), false);\n",
        "\n",
        "    // 6. Vectorized (float4 + Warp Red)\n",
        "    runKernel(\"MaxRed5 (Vec4 + Warp)\", MaxRed5, gridSizePersistent, blockSize, (blockSize/32) * sizeof(float), false);\n",
        "\n",
        "    printf(\"--------------------------------------------------------------------------------\\n\");\n",
        "\n",
        "    // Clean up\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "    CUDA_CHECK(cudaFree(d_input));\n",
        "    CUDA_CHECK(cudaFree(d_output));\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W_T-Nd_nOwJH"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_70 -o MaxReduction MaxReduction.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tre8A7qOzVl",
        "outputId": "85d80c33-c5e3-4efd-ef9f-df61434b92cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating CPU reference for N=16777216....\n",
            "CPU result: 123456.00 (Time : 42.2390 of ms)\n",
            "\n",
            "Kernel Performance Comparison:\n",
            "--------------------------------------------------------------------------------\n",
            "Naive (Grid Stride)       | Grid: 2048 | Time: 19850.488 ms | Result: 123456.00 | PASS\n",
            "Interleaved 1 (Mod)       | Grid: 65536 | Time:  1.108 ms | Result: 123456.00 | PASS\n",
            "Interleaved 2 (Strided)   | Grid: 65536 | Time:  0.841 ms | Result: 123456.00 | PASS\n",
            "Sequential                | Grid: 65536 | Time:  0.678 ms | Result: 123456.00 | PASS\n",
            "MaxRed4 (Grid Stride)     | Grid: 2048 | Time:  0.286 ms | Result: 123456.00 | PASS\n",
            "MaxRed5 (Vec4 + Warp)     | Grid: 2048 | Time:  0.283 ms | Result: 123456.00 | PASS\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!./MaxReduction"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
