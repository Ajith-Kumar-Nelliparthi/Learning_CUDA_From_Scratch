{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_at6h7vZWjXn",
        "outputId": "d34069b1-85ca-4480-e2da-653036f93327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting Sigmoid.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile Sigmoid.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <math.h>\n",
        "\n",
        "__global__ void Sigmoid(const float *A, float *O, int N){\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < N){\n",
        "        O[idx] = 1.0f / (1.0f + expf(-A[idx]));\n",
        "    }\n",
        "}\n",
        "\n",
        "void solve(const float *A, float *O, int N){\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocks = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    Sigmoid<<<blocks, threadsPerBlock>>>(A, O, N);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess){\n",
        "        printf(\"Cuda Error: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    const int N = 1 << 24;\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    float *h_i = new float[N];\n",
        "    float *h_o = new float[N];\n",
        "\n",
        "    for (int i=0; i< N; i++){\n",
        "        h_i[i] = -0.5f + 10.0f * static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    float *d_i, *d_o;\n",
        "    cudaMalloc((void **)&d_i, size);\n",
        "    cudaMalloc((void **)&d_o, size);\n",
        "\n",
        "    cudaMemcpy(d_i, h_i, size, cudaMemcpyHostToDevice);\n",
        "    solve(d_i, d_o, N);\n",
        "    cudaMemcpy(h_o, d_o, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"First 5 sigmoid outputs: \\n\");\n",
        "\tfor (int i = 0; i < 5 && i < N; i++) {\n",
        "\t\tprintf(\"y[%d] = %f (x[%d] = %f)\\n\", i, h_o[i], i, h_i[i]);\n",
        "\t}\n",
        "\n",
        "\tcudaFree(d_i);\n",
        "\tcudaFree(d_o);\n",
        "\tdelete[] h_i;\n",
        "    delete[] h_o;\n",
        "\treturn 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEBTyVl1xGcZ"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 Sigmoid.cu -o Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ubI-opSxJ9g",
        "outputId": "b0ab5a88-ce5d-4b2d-c904-42b615f2efe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 sigmoid outputs: \n",
            "y[0] = 0.999630 (x[0] = 7.901877)\n",
            "y[1] = 0.969047 (x[1] = 3.443829)\n",
            "y[2] = 0.999345 (x[2] = 7.330992)\n",
            "y[3] = 0.999438 (x[3] = 7.484400)\n",
            "y[4] = 0.999819 (x[4] = 8.616474)\n"
          ]
        }
      ],
      "source": [
        "!./Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDnBS8gqx2gu",
        "outputId": "539cbd3e-b9f7-4261-9766-e7df07934291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting Relu.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile Relu.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <math.h>\n",
        "\n",
        "__global__ void Relu(const float *I, float *O, int N){\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    for (int i=idx; i<N; i+= blockDim.x * gridDim.x){\n",
        "        O[i] = fmaxf(0, I[i]);\n",
        "    }\n",
        "}\n",
        "void solve(const float *I, float *O, int N){\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocks = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    Relu<<<blocks, threadsPerBlock>>>(I, O, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess){\n",
        "        printf(\"Cuda Error %s\\n:\", cudaGetErrorString(err));\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    const int N = 1 << 20;\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    float *h_i = new float[N];\n",
        "    float *h_o = new float[N];\n",
        "\n",
        "    for (int t=0; t< N; t++){\n",
        "        h_i[t] = -5.0f + 10.0f * static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    float *d_i, *d_o;\n",
        "    cudaMalloc((void **)&d_i, size);\n",
        "    cudaMalloc((void **)&d_o, size);\n",
        "\n",
        "    cudaMemcpy(d_i, h_i, size, cudaMemcpyHostToDevice);\n",
        "    solve(d_i, d_o, N);\n",
        "    cudaMemcpy(h_o, d_o, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"First 5 ReLU outputs: \\n\");\n",
        "\tfor (int i = 0; i < 5 && i < N; i++) {\n",
        "\t\tprintf(\"O[%d] = %f (I[%d] = %f)\\n\", i, h_o[i], i, h_i[i]);\n",
        "\t}\n",
        "\n",
        "\tcudaFree(d_i);\n",
        "\tcudaFree(d_o);\n",
        "\tdelete[] h_i;\n",
        "    delete[] h_o;\n",
        "\treturn 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sBT0Q4I25Kb"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 Relu.cu -o Relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW0Jbeif2-65",
        "outputId": "6059c9fc-d451-43a3-868b-3906f73ea1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 ReLU outputs: \n",
            "O[0] = 3.401877 (I[0] = 3.401877)\n",
            "O[1] = 0.000000 (I[1] = -1.056171)\n",
            "O[2] = 2.830992 (I[2] = 2.830992)\n",
            "O[3] = 2.984400 (I[3] = 2.984400)\n",
            "O[4] = 4.116474 (I[4] = 4.116474)\n"
          ]
        }
      ],
      "source": [
        "!./Relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9DyHo0bjeXA",
        "outputId": "4c7ba1b4-1dd1-4995-8b10-22d766921a56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing l2Norm.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile l2Norm.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "\n",
        "#define CUDA_CHECK(call) \\\n",
        "    do { \\\n",
        "        cudaError_t err = call; \\\n",
        "        if (err != cudaSuccess) { \\\n",
        "            fprintf(stderr, \"CUDA CHECK ERROR %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(EXIT_FAILURE); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "// warp reduction using shuffle\n",
        "static inline __device__ float warpReduceSum(float val){\n",
        "    for (int offset = 16; offset > 0; offset >>= 1){\n",
        "        val += __shfl_down_sync(0xffffffff, val, offset);\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "// Block Reduction using shared Memory\n",
        "__device__ __forceinline__ float blockReduceSum(float val, float* shared) {\n",
        "    int lane = threadIdx.x % 32;\n",
        "    int wid = threadIdx.x / 32;\n",
        "\n",
        "    // Warp-level reduction\n",
        "    val = warpReduceSum(val);\n",
        "\n",
        "    // Write reduced value to shared memory if first lane in warp\n",
        "    if (lane == 0) shared[wid] = val;\n",
        "    __syncthreads();\n",
        "\n",
        "    // First warp reduces per-warp sums\n",
        "    if (wid == 0) {\n",
        "        val = (lane < (blockDim.x + 31) / 32) ? shared[lane] : 0.0f;\n",
        "        val = warpReduceSum(val);\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "\n",
        "// Kernel-1:- Sum of Squares using vectorized loads (float4) + register accumulation\n",
        "__global__ void L2SquaredSumKernel(const float * __restrict__ input, float *globalsum, int N){\n",
        "    extern __shared__ float sdata[];\n",
        "\n",
        "    float acc = 0.0f;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    // Vectorized load (float4 = 4 floats)\n",
        "    int vecN = N / 4;\n",
        "    const float4 *vinput = reinterpret_cast<const float4*>(input);\n",
        "\n",
        "    // process 4 elements at once\n",
        "    for (int i = idx; i < vecN; i += stride) {\n",
        "        float4 v = vinput[i];\n",
        "        acc += v.x * v.x;\n",
        "        acc += v.y * v.y;\n",
        "        acc += v.z * v.z;\n",
        "        acc += v.w * v.w;\n",
        "    }\n",
        "\n",
        "    // Handle remaining elements\n",
        "    for (int i = vecN * 4 + idx; i < N; i += stride) {\n",
        "        float x = input[i];\n",
        "        acc += x * x;\n",
        "    }\n",
        "    // Reduce within block\n",
        "    acc = blockReduceSum(acc, sdata);\n",
        "\n",
        "    if (threadIdx.x == 0){\n",
        "        atomicAdd(globalsum, acc);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel-2:- Normalized using Pre-computed L2 Norm(scalar), vectorized stores\n",
        "__global__ void NormalizeKernel(const float * __restrict__ input, float * __restrict__ output, float invNorm, int N){\n",
        "    int vecN = N / 4;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    const float4 *vinput = reinterpret_cast<const float4*>(input);\n",
        "    float4 *voutput = reinterpret_cast<float4*>(output);\n",
        "\n",
        "    for (int i = idx; i < vecN; i += stride) {\n",
        "        float4 v = vinput[i];\n",
        "        v.x *= invNorm;\n",
        "        v.y *= invNorm;\n",
        "        v.z *= invNorm;\n",
        "        v.w *= invNorm;\n",
        "        voutput[i] = v;\n",
        "    }\n",
        "    // tail elements\n",
        "    for (int i = vecN * 4 + idx; i < N; i += stride) {\n",
        "        output[i] = input[i] * invNorm;\n",
        "    }\n",
        "}\n",
        "\n",
        "void l2_normalize_cuda(const float* d_input, float* d_output, int N) {\n",
        "    const int threadsPerBlock = 256;\n",
        "    const int blocks = (N + threadsPerBlock * 4 - 1) / (threadsPerBlock * 4);\n",
        "\n",
        "    float *d_sum = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_sum, sizeof(float)));\n",
        "    CUDA_CHECK(cudaMemset(d_sum, 0, sizeof(float)));\n",
        "\n",
        "    // Shared memory: one float per warp\n",
        "    int warpsPerBlock = (threadsPerBlock + 31) / 32;\n",
        "    size_t smemBytes = warpsPerBlock * sizeof(float);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "\n",
        "    // Step 1: Compute sum of squares\n",
        "    L2SquaredSumKernel<<<blocks, threadsPerBlock, smemBytes>>>(d_input, d_sum, N);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // Step 2: Compute norm and inverse\n",
        "    float h_sum = 0.0f;\n",
        "    CUDA_CHECK(cudaMemcpy(&h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaFree(d_sum));\n",
        "\n",
        "    float L2_norm = sqrtf(h_sum);\n",
        "    float invNorm = (L2_norm > 1e-12f) ? 1.0f / L2_norm : 0.0f;\n",
        "\n",
        "    // Step 3: Normalize\n",
        "    NormalizeKernel<<<blocks, threadsPerBlock>>>(d_input, d_output, invNorm, N);\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float elapsed_ms = 0.0f;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&elapsed_ms, start, stop));\n",
        "    printf(\"L2 Normalization GPU time: %.3f ms (N = %d, %.2f M elements/sec)\\n\",\n",
        "           elapsed_ms, N, N / (elapsed_ms * 1e3f));\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1 << 20; // 1048576\n",
        "    size_t bytes = N * sizeof(float);\n",
        "\n",
        "    float* h_input = (float*)malloc(bytes);\n",
        "    float* h_output = (float*)malloc(bytes);\n",
        "\n",
        "    if (!h_input || !h_output) { fprintf(stderr, \"host alloc failed\\n\"); return 1; }\n",
        "\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_input[i] = static_cast<float>(rand()) / RAND_MAX * 2.0f - 1.0f;\n",
        "    }\n",
        "\n",
        "    float *d_input = nullptr, *d_output = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_input, bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&d_output, bytes));\n",
        "    CUDA_CHECK(cudaMemcpy(d_input, h_input, bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    l2_normalize_cuda(d_input, d_output, N);\n",
        "\n",
        "    // Copy back result\n",
        "    CUDA_CHECK(cudaMemcpy(h_output, d_output, bytes, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // Verify correctness: ||output|| should be ~1.0\n",
        "    double sum_sq = 0.0;\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        sum_sq += h_output[i] * h_output[i];\n",
        "    }\n",
        "    printf(\"\\nVerification: L2 norm of output = %.8f (should be ~1.0)\\n\", sqrt(sum_sq));\n",
        "\n",
        "    printf(\"First 10 values:\\n\");\n",
        "    for (int i = 0; i < 10 && i < N; ++i) {\n",
        "        printf(\"in[%d] = %8.5f  -> out[%d] = %8.5f\\n\", i, h_input[i], i, h_output[i]);\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    CUDA_CHECK(cudaFree(d_input));\n",
        "    CUDA_CHECK(cudaFree(d_output));\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    CUDA_CHECK(cudaDeviceReset());\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd2KOEMmm93v"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 l2Norm.cu -o l2Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grlzaJgmnDYF",
        "outputId": "de633050-8c39-45c9-d10c-95df948c6d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L2 Normalization GPU time: 0.397 ms (N = 1048576, 2639.39 M elements/sec)\n",
            "\n",
            "Verification: L2 norm of output = 0.99999999 (should be ~1.0)\n",
            "First 10 values:\n",
            "in[0] =  0.68038  -> out[0] =  0.00115\n",
            "in[1] = -0.21123  -> out[1] = -0.00036\n",
            "in[2] =  0.56620  -> out[2] =  0.00096\n",
            "in[3] =  0.59688  -> out[3] =  0.00101\n",
            "in[4] =  0.82329  -> out[4] =  0.00139\n",
            "in[5] = -0.60490  -> out[5] = -0.00102\n",
            "in[6] = -0.32955  -> out[6] = -0.00056\n",
            "in[7] =  0.53646  -> out[7] =  0.00091\n",
            "in[8] = -0.44445  -> out[8] = -0.00075\n",
            "in[9] =  0.10794  -> out[9] =  0.00018\n"
          ]
        }
      ],
      "source": [
        "!./l2Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvipB1DWnFd1",
        "outputId": "1790af91-2af1-40a0-e58d-59bb40236d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing OptmizedL2Norm.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile OptmizedL2Norm.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define CUDA_CHECK(call) \\\n",
        "    do { \\\n",
        "        cudaError_t err = call; \\\n",
        "        if (err != cudaSuccess){ \\\n",
        "            fprintf(stderr, \"CUDA CHECK ERROR %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(EXIT_FAILURE); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "// warp-reduction\n",
        "__device__  __forceinline__ float warpReduceSum(float val){\n",
        "    for (int offset= 16; offset>0; offset >>= 1){\n",
        "        val += __shfl_down_sync(0xffffffff, val, offset);\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "// block reduction\n",
        "__device__ __forceinline__ float blockReduceSum(float val, float* shared){\n",
        "    int lane = threadIdx.x % 32;\n",
        "    int wid = threadIdx.x / 32;\n",
        "\n",
        "    // warp-level reduction\n",
        "    val = warpReduceSum(val);\n",
        "\n",
        "    // store warp results in shared memory\n",
        "    if (lane == 0) shared[wid] = val;\n",
        "    __syncthreads();\n",
        "\n",
        "    // the final reduction\n",
        "    if (wid == 0){\n",
        "        val = (lane < (blockDim.x + 31) / 32) ? shared[lane] : 0.0f;\n",
        "        val = warpReduceSum(val);\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "// kernel-1: sum of squares\n",
        "__global__ void L2SquaredKernel(const float * __restrict__ input, float *globalsum, int N){\n",
        "    extern __shared__ float sdata[];\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "    float acc = 0.0f;\n",
        "\n",
        "    // vectorized load\n",
        "    int vecN = N / 4;\n",
        "    const float4 *vinput = reinterpret_cast<const float4*>(input);\n",
        "\n",
        "    // process 4 elements at once\n",
        "    for (int i=idx; i<vecN; i+= stride){\n",
        "        float4 v = vinput[i];\n",
        "        acc += v.x * v.x;\n",
        "        acc += v.y * v.y;\n",
        "        acc += v.z * v.z;\n",
        "        acc += v.w * v.w;\n",
        "    }\n",
        "    // handle remaining elements\n",
        "    for (int tail = vecN * 4 + idx; tail<N; tail+= stride){\n",
        "        float x = input[tail];\n",
        "        acc += x * x;\n",
        "    }\n",
        "    // reduce within block\n",
        "    acc = blockReduceSum(acc, sdata);\n",
        "    if (threadIdx.x == 0){\n",
        "        atomicAdd(globalsum, acc);\n",
        "    }\n",
        "}\n",
        "\n",
        "// kernel-2\n",
        "__global__ void NormalizedKernel(const float * __restrict__ input, float * __restrict__ output, float invNorm, int N){\n",
        "    int vecN = N / 4;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    const float4 *vinput = reinterpret_cast<const float4*>(input);\n",
        "    float4 *voutput = reinterpret_cast<float4*>(output);\n",
        "\n",
        "    for (int i = idx; i<vecN; i+=stride){\n",
        "        float4 v = vinput[i];\n",
        "        v.x *= invNorm;\n",
        "        v.y *= invNorm;\n",
        "        v.z *= invNorm;\n",
        "        v.w *= invNorm;\n",
        "        voutput[i] = v;\n",
        "    }\n",
        "    for (int tail = vecN * 4 + idx; tail<N; tail+= stride){\n",
        "        output[tail] = input[tail] * invNorm;\n",
        "    }\n",
        "}\n",
        "\n",
        "void L2Normalize_cuda_kernel(const float *d_input, float *d_output, int N){\n",
        "    const int blockSizes[] = {128, 256, 512};\n",
        "    int numTests = 3;\n",
        "\n",
        "    float best_time = 1e9;\n",
        "    int best_block = 256;\n",
        "\n",
        "    printf(\"Tuning L2 normalization on N = %d (%.2f M elements)\\n\\n\", N, N/1e6f);\n",
        "    printf(\"%-8s %8s %12s %12s\\n\", \"Block\", \"Grid\", \"Time [ms]\", \"GB/s\");\n",
        "\n",
        "    for (int t=0; t<numTests; t++){\n",
        "        int threadsPerBlock = blockSizes[t];\n",
        "\n",
        "        int min_grid = (N + threadsPerBlock*4 - 1) / (threadsPerBlock * 4);\n",
        "        int grid = max(min_grid, 1);\n",
        "        grid = min(grid, 65535);\n",
        "\n",
        "        float *d_sum = nullptr;\n",
        "        CUDA_CHECK(cudaMalloc(&d_sum, sizeof(float)));\n",
        "\n",
        "        int warps_per_block = (threadsPerBlock + 31) / 32;\n",
        "        size_t bytes = warps_per_block * sizeof(float);\n",
        "\n",
        "        cudaEvent_t start, stop;\n",
        "        CUDA_CHECK(cudaEventCreate(&start));\n",
        "        CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "        // warm-up\n",
        "        CUDA_CHECK(cudaMemset(d_sum, 0, sizeof(float)));\n",
        "        L2SquaredKernel<<<grid, threadsPerBlock, bytes>>>(d_input, d_sum, N);\n",
        "        CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "        CUDA_CHECK(cudaEventRecord(start));\n",
        "        CUDA_CHECK(cudaMemset(d_sum, 0, sizeof(float)));\n",
        "        L2SquaredKernel<<<grid, threadsPerBlock, bytes>>>(d_input, d_sum, N);\n",
        "        CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "        float h_sum = 0.0f;\n",
        "        CUDA_CHECK(cudaMemcpy(&h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost));\n",
        "        float norm = sqrtf(h_sum);\n",
        "        float invNorm = (norm > 1e-12f) ? 1.0f / norm : 0.0f;\n",
        "\n",
        "        NormalizedKernel<<<grid, threadsPerBlock>>>(d_input, d_output, invNorm, N);\n",
        "        CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "        CUDA_CHECK(cudaEventRecord(stop));\n",
        "        CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "        float ms = 0.0f;\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
        "\n",
        "        float gbs = (2.0f * N * sizeof(float)) / (ms * 1e6f); // read + write\n",
        "\n",
        "        printf(\"%-8d %8d %12.3f %12.2f\\n\", threadsPerBlock, grid, ms, gbs);\n",
        "\n",
        "        if (ms < best_time) {\n",
        "            best_time = ms;\n",
        "            best_block  = threadsPerBlock;\n",
        "        }\n",
        "\n",
        "        CUDA_CHECK(cudaFree(d_sum));\n",
        "        CUDA_CHECK(cudaEventDestroy(start));\n",
        "        CUDA_CHECK(cudaEventDestroy(stop));\n",
        "    }\n",
        "    printf(\"\\nBest Block: block size = %d (%.3f ms, %.2f GB/s)\\n\", best_block, best_time,\n",
        "           (2.0f*N*sizeof(float)/(best_time*1e6f)));\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1 << 24;\n",
        "    size_t bytes = N * sizeof(float);\n",
        "\n",
        "    float *h_in  = (float*)malloc(bytes);\n",
        "    float *h_out = (float*)malloc(bytes);\n",
        "\n",
        "    for (int i = 0; i < N; ++i)\n",
        "        h_in[i] = (float)rand() / RAND_MAX * 2.0f - 1.0f;\n",
        "\n",
        "    float *d_in = nullptr, *d_out = nullptr;\n",
        "    CUDA_CHECK(cudaMalloc(&d_in,  bytes));\n",
        "    CUDA_CHECK(cudaMalloc(&d_out, bytes));\n",
        "    CUDA_CHECK(cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice));\n",
        "\n",
        "    L2Normalize_cuda_kernel(d_in, d_out, N);\n",
        "\n",
        "    // copy back and verify\n",
        "    CUDA_CHECK(cudaMemcpy(h_out, d_out, bytes, cudaMemcpyDeviceToHost));\n",
        "    double norm2 = 0.0;\n",
        "    for (int i = 0; i < N; ++i) norm2 += h_out[i] * h_out[i];\n",
        "    printf(\"\\nFinal output L2 norm = %.9f (should be ≈1.0)\\n\", sqrt(norm2));\n",
        "\n",
        "    // Cleanup\n",
        "    CUDA_CHECK(cudaFree(d_in));\n",
        "    CUDA_CHECK(cudaFree(d_out));\n",
        "    free(h_in); free(h_out);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO0O5FDTQmNl"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 OptmizedL2Norm.cu -o OptmizedL2Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dFXd2QZQnmK",
        "outputId": "fab7eea4-9d36-488b-e385-9b0338ceb106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning L2 normalization on N = 16777216 (16.78 M elements)\n",
            "\n",
            "Block        Grid    Time [ms]         GB/s\n",
            "128         32768        0.885       151.65\n",
            "256         16384        0.878       152.90\n",
            "512          8192        0.890       150.89\n",
            "\n",
            "Best Block: block size = 256 (0.878 ms, 152.90 GB/s)\n",
            "\n",
            "Final output L2 norm = 1.000001015 (should be ≈1.0)\n"
          ]
        }
      ],
      "source": [
        "!./OptmizedL2Norm"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
