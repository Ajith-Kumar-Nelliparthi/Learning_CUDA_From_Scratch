{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayv5-zaSSYe8",
        "outputId": "8761e904-a77c-4a6a-d48d-36476aaa9900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting reduce_v2.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile reduce_v2.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_CHECK(call) \\\n",
        "    do { \\\n",
        "        cudaError_t err = call; \\\n",
        "        if (err != cudaSuccess) { \\\n",
        "            fprintf(stderr, \"CUDA CHECK ERROR %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(EXIT_FAILURE); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "\n",
        "__inline__ __device__ float warpReuce(float val){\n",
        "    for (int offset=16; offset >0; offset >>= 1){\n",
        "        val += __shfl_down_sync(0xffffffff, val, offset);\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "__global__ void reduce_v2(const float *i, float *o, int N){\n",
        "    extern __shared__ float sdata[];\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "    int grid = blockDim.x * gridDim.x;\n",
        "\n",
        "    float sum = 0.0f;\n",
        "    while (idx < N){\n",
        "        sum += i[idx];\n",
        "        if (idx + blockDim.x < N){\n",
        "            sum += i[idx + blockDim.x];\n",
        "        }\n",
        "        idx += grid;\n",
        "    }\n",
        "    sdata[tid] = sum;\n",
        "    __syncthreads();\n",
        "\n",
        "    // block reduction\n",
        "    for (int s=blockDim.x/2; s >= 32; s >>= 1){\n",
        "        if (tid < s){\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // warp reduction\n",
        "    float val = sdata[tid];\n",
        "    if (tid < 32){\n",
        "        val = warpReuce(val);\n",
        "    }\n",
        "\n",
        "    if (tid == 0){\n",
        "        o[blockIdx.x] = val;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    printf(\"=== CUDA Reduction Perfomance Test ===\\n\\n\");\n",
        "\n",
        "    int N = 1 << 20;\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    const int blockSizes[] = {128, 256, 512};\n",
        "    const int numTests = 3;\n",
        "\n",
        "    float *h_a = (float *)malloc(size);\n",
        "    for (int i=0; i<N; i++){\n",
        "        h_a[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    float *d_a, *d_b;\n",
        "    printf(\"Allocating GPU Memory\\n\");\n",
        "    CUDA_CHECK(cudaMalloc((void **)&d_a, size));\n",
        "\n",
        "    int maxBlocks = (N + 128 * 2 - 1) / (128 * 2);\n",
        "    CUDA_CHECK(cudaMalloc((void **)&d_b, maxBlocks * sizeof(float)));\n",
        "    printf(\" d_a: %.2f MB\\n\", size / (1024.0*1024.0));\n",
        "    printf(\" d_b: %.2f MB (max %d blocks)\\n\\n\", (maxBlocks * sizeof(float)) / (1024.0*1024.0), maxBlocks);\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice));\n",
        "    printf(\" Transferred: %.2f MB \\n\\n\", size / (1024.0*1024.0));\n",
        "\n",
        "    // warmup run\n",
        "    reduce_v2<<<512, 256, 256 * sizeof(float)>>>(d_a, d_b, N);\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    printf(\"Running Tests...\\n\");\n",
        "    // overall timing\n",
        "    cudaEvent_t overall_start, overall_stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&overall_start));\n",
        "    CUDA_CHECK(cudaEventCreate(&overall_stop));\n",
        "    CUDA_CHECK(cudaEventRecord(overall_start));\n",
        "\n",
        "    for (int t=0; t<numTests; t++){\n",
        "        int threadsPerBlock = blockSizes[t];\n",
        "        int blocks = (N + threadsPerBlock * 2 - 1) / (threadsPerBlock * 2);\n",
        "\n",
        "        cudaEvent_t start, stop;\n",
        "        CUDA_CHECK(cudaEventCreate(&start));\n",
        "        CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "        CUDA_CHECK(cudaEventRecord(start));\n",
        "        reduce_v2<<<blocks, threadsPerBlock, threadsPerBlock * sizeof(float)>>>(d_a, d_b, N);\n",
        "        CUDA_CHECK(cudaEventRecord(stop));\n",
        "        CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "        float kernel_time = 0;\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&kernel_time, start, stop));\n",
        "\n",
        "        // copy results device to host\n",
        "        float *h_b = (float *)malloc(blocks * sizeof(float));\n",
        "        cudaEvent_t copy_start, copy_stop;\n",
        "        CUDA_CHECK(cudaEventCreate(&copy_start));\n",
        "        CUDA_CHECK(cudaEventCreate(&copy_stop));\n",
        "        CUDA_CHECK(cudaEventRecord(copy_start));\n",
        "        CUDA_CHECK(cudaMemcpy(h_b, d_b, blocks * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "        CUDA_CHECK(cudaEventRecord(copy_stop));\n",
        "        CUDA_CHECK(cudaEventSynchronize(copy_stop));\n",
        "\n",
        "        float copy_time = 0;\n",
        "        CUDA_CHECK(cudaEventElapsedTime(&copy_time, copy_start, copy_stop));\n",
        "\n",
        "        // total sum\n",
        "        float final_sum = 0;\n",
        "        for (int i=0; i<blocks; i++){\n",
        "            final_sum += h_b[i];\n",
        "        }\n",
        "        printf(\"%-15d %-10d %-15.4f %-15.4f %.0f\\n\",\n",
        "            threadsPerBlock, blocks, kernel_time, kernel_time + copy_time, final_sum);\n",
        "        free(h_b);\n",
        "        CUDA_CHECK(cudaEventDestroy(start));\n",
        "        CUDA_CHECK(cudaEventDestroy(stop));\n",
        "        CUDA_CHECK(cudaEventDestroy(copy_start));\n",
        "        CUDA_CHECK(cudaEventDestroy(copy_stop));\n",
        "    }\n",
        "    CUDA_CHECK(cudaEventRecord(overall_stop));\n",
        "    CUDA_CHECK(cudaEventSynchronize(overall_stop));\n",
        "\n",
        "    float total_time = 0;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&total_time, overall_start, overall_stop));\n",
        "\n",
        "    printf(\"\\nTotal time for all 3 tests: %.4f ms\\n\", total_time);\n",
        "    printf(\"\\n=== Performance Summary ===\\n\");\n",
        "    printf(\"Data size: %d elements (%.2f MB)\\n\", N, size / (1024.0*1024.0));\n",
        "    printf(\"Average time per test: %.4f ms\\n\", total_time / numTests);\n",
        "\n",
        "    // Cleanup\n",
        "    CUDA_CHECK(cudaFree(d_a));\n",
        "    CUDA_CHECK(cudaFree(d_b));\n",
        "    free(h_a);\n",
        "    CUDA_CHECK(cudaEventDestroy(overall_start));\n",
        "    CUDA_CHECK(cudaEventDestroy(overall_stop));\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dRwjQvT6ZZP9"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 reduce_v2.cu -o reduce_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuUKmN4GZfKT",
        "outputId": "132e0a13-476d-4478-daac-121fe1ebfa6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CUDA Reduction Perfomance Test ===\n",
            "\n",
            "Allocating GPU Memory\n",
            " d_a: 4.00 MB\n",
            " d_b: 0.02 MB (max 4096 blocks)\n",
            "\n",
            " Transferred: 4.00 MB \n",
            "\n",
            "Running Tests...\n",
            "128             4096       0.0492          0.0841          2097024\n",
            "256             2048       0.0575          0.0749          2096896\n",
            "512             1024       0.0649          0.0790          2096640\n",
            "\n",
            "Total time for all 3 tests: 0.3930 ms\n",
            "\n",
            "=== Performance Summary ===\n",
            "Data size: 1048576 elements (4.00 MB)\n",
            "Average time per test: 0.1310 ms\n"
          ]
        }
      ],
      "source": [
        "!./reduce_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8dIDx6cZmYr",
        "outputId": "90c7e878-a6f1-415f-f0cd-a9581a55c0c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting data...\n",
            "=== CUDA Reduction Perfomance Test ===\n",
            "\n",
            "Allocating GPU Memory\n",
            " d_a: 4.00 MB\n",
            " d_b: 0.02 MB (max 4096 blocks)\n",
            "\n",
            " Transferred: 4.00 MB \n",
            "\n",
            "Running Tests...\n",
            "128             4096       0.0585          0.0925          2097024\n",
            "256             2048       0.0644          0.0836          2096896\n",
            "512             1024       0.0721          0.0915          2096640\n",
            "\n",
            "Total time for all 3 tests: 0.4283 ms\n",
            "\n",
            "=== Performance Summary ===\n",
            "Data size: 1048576 elements (4.00 MB)\n",
            "Average time per test: 0.1428 ms\n",
            "Generating '/tmp/nsys-report-3e47.qdstrm'\n",
            "[1/8] [========================100%] my_timeline_report.nsys-rep\n",
            "\u001b[2KProcessing 1287 events: \n",
            "[2/8] [========================100%] my_timeline_report.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /content/my_timeline_report.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  --------  -----------  ------------  ----------------------\n",
            "     84.8      382,936,520         13  29,456,655.4  2,581,136.0     1,573  282,929,069  77,244,768.2  poll                  \n",
            "     14.3       64,344,258        529     121,633.8     15,853.0     1,246   17,319,614     937,628.7  ioctl                 \n",
            "      0.4        1,897,536         31      61,210.8     11,204.0     8,189    1,206,654     213,498.2  mmap64                \n",
            "      0.2          919,667         10      91,966.7     60,751.5    40,048      365,731      97,088.7  sem_timedwait         \n",
            "      0.1          425,801         49       8,689.8      8,090.0     3,512       25,705       3,650.4  open64                \n",
            "      0.0          215,920         39       5,536.4      3,246.0     1,372       38,074       6,979.5  fopen                 \n",
            "      0.0          183,536         15      12,235.7      6,008.0     1,728       74,738      18,414.3  mmap                  \n",
            "      0.0          131,193          1     131,193.0    131,193.0   131,193      131,193           0.0  pthread_cond_wait     \n",
            "      0.0           94,368          2      47,184.0     47,184.0    41,894       52,474       7,481.2  pthread_create        \n",
            "      0.0           73,110         12       6,092.5      5,623.0     1,365       18,054       4,057.0  write                 \n",
            "      0.0           44,538          6       7,423.0      5,962.5     1,576       17,037       5,231.8  open                  \n",
            "      0.0           39,104          6       6,517.3      5,511.0     2,867       14,082       3,875.4  munmap                \n",
            "      0.0           37,140         21       1,768.6      1,148.0     1,013        6,088       1,382.6  fclose                \n",
            "      0.0           36,036          1      36,036.0     36,036.0    36,036       36,036           0.0  fgets                 \n",
            "      0.0           21,928         13       1,686.8      1,455.0     1,023        3,481         678.8  read                  \n",
            "      0.0           17,635          2       8,817.5      8,817.5     6,683       10,952       3,018.6  socket                \n",
            "      0.0           16,869          7       2,409.9      1,384.0     1,178        5,070       1,502.4  close                 \n",
            "      0.0           14,111          3       4,703.7      5,046.0     2,800        6,265       1,757.7  pipe2                 \n",
            "      0.0            8,757          1       8,757.0      8,757.0     8,757        8,757           0.0  connect               \n",
            "      0.0            5,299          2       2,649.5      2,649.5     2,252        3,047         562.1  fwrite                \n",
            "      0.0            4,961          1       4,961.0      4,961.0     4,961        4,961           0.0  pthread_cond_broadcast\n",
            "      0.0            1,572          1       1,572.0      1,572.0     1,572        1,572           0.0  bind                  \n",
            "      0.0            1,097          1       1,097.0      1,097.0     1,097        1,097           0.0  fcntl                 \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)   Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  ------------  --------  ----------  ------------  ----------------------\n",
            "     98.1       97,548,128          2  48,774,064.0  48,774,064.0   106,696  97,441,432  68,826,051.9  cudaMalloc            \n",
            "      1.0        1,008,413          4     252,103.3      25,130.5    17,814     940,338     458,871.3  cudaMemcpy            \n",
            "      0.4          349,418          2     174,709.0     174,709.0   135,809     213,609      55,012.9  cudaFree              \n",
            "      0.2          208,217          4      52,054.3      10,626.5     9,087     177,877      83,885.9  cudaLaunchKernel      \n",
            "      0.2          190,950          7      27,278.6       5,998.0     4,577      64,474      27,413.2  cudaEventSynchronize  \n",
            "      0.0           46,113         14       3,293.8       2,776.5     1,879       8,173       1,798.0  cudaEventRecord       \n",
            "      0.0           23,690          1      23,690.0      23,690.0    23,690      23,690           0.0  cudaDeviceSynchronize \n",
            "      0.0           21,377         14       1,526.9         859.0       391       9,387       2,319.3  cudaEventCreate       \n",
            "      0.0           15,609         14       1,114.9         664.0       429       3,289         849.1  cudaEventDestroy      \n",
            "      0.0              858          1         858.0         858.0       858         858           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                   Name                 \n",
            " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------------------\n",
            "    100.0          179,131          4  44,782.8  48,447.0    22,239    59,998     16,142.3  reduce_v2(const float *, float *, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
            " --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ----------------------------\n",
            "     99.0          752,397      1  752,397.0  752,397.0   752,397   752,397          0.0  [CUDA memcpy Host-to-Device]\n",
            "      1.0            7,616      3    2,538.7    2,528.0     1,920     3,168        624.1  [CUDA memcpy Device-to-Host]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
            "      4.194      1     4.194     4.194     4.194     4.194        0.000  [CUDA memcpy Host-to-Device]\n",
            "      0.029      3     0.010     0.008     0.004     0.016        0.006  [CUDA memcpy Device-to-Host]\n",
            "\n",
            "Generated:\n",
            "\t/content/my_timeline_report.nsys-rep\n",
            "\t/content/my_timeline_report.sqlite\n"
          ]
        }
      ],
      "source": [
        "!nsys profile --stats=true --trace=cuda,osrt,nvtx --cuda-memory-usage=true --output=my_timeline_report ./reduce_v2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
